{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Word embedding\n",
    "\n",
    "In the second assignment, we are going to experiment with GloVe embeddings.\n",
    "1. Load and study the GloVe vectors (15') \n",
    "  - Nearest_neighbor.  \n",
    "  - Word analogy.  \n",
    "  - Bias in vectors.  \n",
    "2. Train an embedding-based classifier. (10')  \n",
    "  - Use GloVe  \n",
    "  - Use Cohere embedding vectors and compare the performance to GloVe.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and study the GloVe vectors\n",
    "First download the `glove.6B.zip` from [the website](https://nlp.stanford.edu/projects/glove/).  \n",
    "Then, load the vectors using the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load GloVe vectors\n",
    "def load_glove_vectors(file_path):\n",
    "    word_vectors = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_vectors[word] = vector\n",
    "    return word_vectors\n",
    "\n",
    "glove_vectors = load_glove_vectors('Desktop/glove.6B/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbors\n",
    "Implement a \"find nearest neighbors\" function. Look for the 5 nearest neighbors of each of the following words. Print out the found neighbors as well as the similarities. Comment on the findings.  \n",
    "`asdfg, king, queen, book, pen, raise, draft, kellogg`  \n",
    "Note that some of these words might not occur in the vocabulary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'asdfg' not found in vocabulary\n",
      "[('queen', 0.6336469), ('prince', 0.61966234), ('monarch', 0.58996207), ('kingdom', 0.57912666), ('throne', 0.5606488)]\n",
      "[('elizabeth', 0.67714477), ('princess', 0.6356763), ('king', 0.6336469), ('monarch', 0.58141875), ('royal', 0.5430526)]\n",
      "[('books', 0.79862493), ('author', 0.71234983), ('published', 0.6973031), ('novel', 0.69667095), ('memoir', 0.64656407)]\n",
      "[('ballpoint', 0.54257977), ('pens', 0.54234904), ('pencil', 0.46788222), ('ink', 0.45545986), ('le', 0.42909268)]\n",
      "[('raising', 0.78430694), ('raised', 0.7381018), ('raises', 0.7100081), ('increase', 0.61106485), ('interest', 0.5880811)]\n",
      "[('drafted', 0.8233013), ('drafts', 0.608817), ('drafting', 0.5927947), ('pick', 0.53310865), ('proposal', 0.52689034)]\n",
      "[('cereal', 0.46913424), ('halliburton', 0.44785973), ('w.k.', 0.44425425), ('kbr', 0.42964092), ('kellog', 0.40224463)]\n"
     ]
    }
   ],
   "source": [
    "# Nearest neighbor function\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_nearest_neighbors(word, word_vectors, n=5):\n",
    "    if word not in word_vectors:\n",
    "        return f\"'{word}' not found in vocabulary\"\n",
    "    \n",
    "    word_vector = word_vectors[word].reshape(1,-1)\n",
    "    sims = {}\n",
    "    for other_word, vector in word_vectors.items():\n",
    "        if other_word != word:\n",
    "            sim = cosine_similarity(word_vector, vector.reshape(1,-1))[0][0]\n",
    "            sims[other_word] = sim\n",
    "            \n",
    "    nearest_neighbors = sorted(sims.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    return nearest_neighbors\n",
    "\n",
    "\n",
    "for w in [\"asdfg\", \"king\", \"queen\", \"book\", \"pen\", \"raise\", \"draft\", \"kellogg\"]:\n",
    "    print(find_nearest_neighbors(w, glove_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words with the highest similarities are very closely related to the word. For Example, King has the highest similarity to queen and is also very similar to other words that represent royalty or are closely related. This is similar to the word Queen, which is similar to other royalty words and words closely related to queen. Book has some closer similarity values with books which is the word's plural version. Similar occurances are seen with the following words as types of the inputted word, different versions (plural, past tense etc.) are closely related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word analogy\n",
    "What's the 5 words that have the closest vector as the vector $v(king)-v(man)+v(woman)$?  \n",
    "Modify the nearest neighbor function you implemented above. Find the words and the corresponding similarities. Comment on the findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.8065859),\n",
       " ('queen', 0.6896163),\n",
       " ('monarch', 0.5575491),\n",
       " ('throne', 0.5565375),\n",
       " ('princess', 0.5518684)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_word_analogy(word, word_vectors, pair=(\"man\", \"woman\"), n=5):\n",
    "    if word not in word_vectors or pair[0] not in word_vectors or pair[1] not in word_vectors:\n",
    "        return \"One of the words not found in GloVe vocabulary\"\n",
    "    analogy = word_vectors[word] - word_vectors[pair[0]] + word_vectors[pair[1]]\n",
    "    sims = {}\n",
    "    \n",
    "    for other_word, vector in word_vectors.items():\n",
    "        sim = cosine_similarity(analogy.reshape(1,-1), vector.reshape(1,-1))[0][0]\n",
    "        sims[other_word] = sim\n",
    "        \n",
    "    nearest_neighbors = sorted(sims.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    return nearest_neighbors\n",
    "        \n",
    "\n",
    "find_word_analogy(\"king\", glove_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The findings make sense with king and queen as the top results due to the operation transforming the king vector and signifying the relationship between king and queen. similarly, the next result of monarch is closely related to royalty and is a gender neutral term. Throne and princess are also related due to the royalty nature of the terms although they do not have too much relation to gender (other than princess which is related to woman)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias in word vectors\n",
    "Now we will follow the procedure of [Caliskan et al (2018)](https://purehost.bath.ac.uk/ws/portalfiles/portal/168480066/CaliskanEtAl_authors_full.pdf) and compute the following statistic to assess the bias of a word $w$:  \n",
    "$$s(w, A, B) = \\frac{\\textrm{mean}_{a\\in A} \\textrm{cos}(w,a) - \\textrm{mean}_{b\\in B}\\textrm{cos}(w,b)}{\\textrm{stdev}_{x\\in A\\cup B} \\textrm{cos}(w,x)}$$\n",
    "where:  \n",
    "- $A$ and $B$ are the \"attribute words\" of two categories. In this assignment, let's use the following collection:  \n",
    "  `A:` male, man, boy, brother, he, him, his, son  \n",
    "  `B:` female, woman, girl, sister, she, her, hers, daughter    \n",
    "- The operation $\\textrm{cos}(\\cdot, \\cdot)$ computes the cosine similarities between the embedding vectors of the two words.  \n",
    "\n",
    "Compute the statistic for the following occupations. What do the statistic reveal? Comment on the results.  \n",
    "`technician, accountant, supervisor, engineer, worker, doctor, physician, nurse, teacher`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "technician\t 0.1471\n",
      "accountant\t -0.3167\n",
      "supervisor\t -0.8852\n",
      "engineer\t 0.9844\n",
      "worker\t -0.5381\n",
      "doctor\t 0.2340\n",
      "physician\t 0.5511\n",
      "nurse\t -1.4563\n",
      "teacher\t -0.6045\n"
     ]
    }
   ],
   "source": [
    "def analyze_bias(w, A, B, word_vectors):\n",
    "    if w not in word_vectors:\n",
    "        return f\"'{w}' not found in vocabulary\"\n",
    "    bias = (np.mean([cosine_similarity(word_vectors[w].reshape(1,-1), word_vectors[a].reshape(1,-1))[0][0] for a in A]) - np.mean([cosine_similarity(word_vectors[w].reshape(1,-1), word_vectors[b].reshape(1,-1))[0][0] for b in B]))\n",
    "    std = np.std([cosine_similarity(word_vectors[w].reshape(1,-1), word_vectors[x].reshape(1,-1))[0][0] for x in A + B])\n",
    "    return bias/std\n",
    "\n",
    "masculine_words = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"]\n",
    "feminine_words = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \"hers\", \"daughter\"]\n",
    "\n",
    "for w in [\"technician\", \"accountant\", \"supervisor\", \"engineer\", \"worker\", \"doctor\", \"physician\", \"nurse\", \"teacher\"]:\n",
    "    s = analyze_bias(w, masculine_words, feminine_words, glove_vectors)\n",
    "    print(f\"{w}\\t {s:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that have a positive value have a higher association with masculinity such as technician, engineer, doctor, and physician. The words with a negative value have a higher association with femininity such as accountant, supervisior, worker, nurse, and teacher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train embedding-based classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a GloVe-based classifier\n",
    "Train a binary classifier using the GloVe embeddings. This classifier takes the average of the embedded words in a sentence, followed by a MLP with a hidden layer of 100 units. The classifier can be implemented with scikit-learn or pytorch.  \n",
    "Report the classification accuracy on the *validation* set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset  # This dataset is loaded in the same way as Assignment 1.\n",
    "import pandas as pd\n",
    "\n",
    "ds = load_dataset(\"glue\", \"sst2\")\n",
    "train_data = pd.DataFrame(ds[\"train\"])\n",
    "X_train_text = train_data[\"sentence\"]\n",
    "Y_train = train_data[\"label\"]\n",
    "\n",
    "val_data = pd.DataFrame(ds[\"validation\"])\n",
    "X_val_text = val_data[\"sentence\"]\n",
    "Y_val = val_data[\"label\"]\n",
    "\n",
    "test_data = pd.DataFrame(ds[\"test\"])\n",
    "X_test_text = test_data[\"sentence\"]\n",
    "Y_test = test_data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy using GloVe embeddings: 0.7263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def convert_sentence(sentence, word_vectors):\n",
    "    words = sentence.split()\n",
    "    valid_words = [word_vectors[word] for word in words if word in word_vectors]\n",
    "    if len(valid_words) > 0:\n",
    "        return np.mean(valid_words, axis=0)\n",
    "    else:\n",
    "        return np.zeros(300) \n",
    "\n",
    "def train_eval_model(data, glove_vectors):\n",
    "    X_train_text, Y_train, X_val_text, Y_val, X_test_text, Y_test = data \n",
    "    \n",
    "    X_train = np.array([convert_sentence(sentence, glove_vectors) for sentence in X_train_text])\n",
    "    X_val = np.array([convert_sentence(sentence, glove_vectors) for sentence in X_val_text])\n",
    "    X_test = np.array([convert_sentence(sentence, glove_vectors) for sentence in X_test_text])\n",
    "        \n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "    mlp.fit(X_train, Y_train)\n",
    "    \n",
    "    Y_val_pred = mlp.predict(X_val)\n",
    "    val_accuracy = accuracy_score(Y_val, Y_val_pred)\n",
    "    print(f\"Validation Accuracy using GloVe embeddings: {val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "train_eval_model((X_train_text, Y_train, X_val_text, Y_val, X_test_text, Y_test), glove_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Cohere-based classifier\n",
    "Repeat the previous step, with the very important difference of using [Cohere's embedding](https://docs.cohere.com/reference/embed) instead of GloVe.  \n",
    "Report the classification accuracy on the *validation* set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy using Cohere embeddings: 0.9263\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "\n",
    "\n",
    "co = cohere.ClientV2(\"vpMDpYsyvscYIZz3N7OXH8Kh2IrjVAZ3y544KgZB\")\n",
    "\n",
    "#X_train_text = X_train_text[:95]\n",
    "#X_test_text = X_test_text[:95]\n",
    "#X_val_text = X_val_text[:95]\n",
    "#Y_train = Y_train[:95]\n",
    "#Y_test = Y_test[:95]\n",
    "#Y_val = Y_val[:95]\n",
    "\n",
    "\n",
    "def get_cohere_embeddings(sentences, batch_size=95):\n",
    "    embeddings = []\n",
    "    # Cohere API call to get embeddings for a batch of sentences\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i + batch_size]\n",
    "        response = co.embed(texts=batch, model=\"embed-english-v3.0\", input_type='classification', embedding_types=['float'] )\n",
    "        embeddings.extend(response.embeddings.float_)\n",
    "    return embeddings\n",
    "\n",
    "def train_eval_model_cohere(data):\n",
    "    X_train_text, Y_train, X_val_text, Y_val, X_test_text = data\n",
    "    \n",
    "    # Get Cohere embeddings for train, validation, and test sets\n",
    "    X_train = get_cohere_embeddings(X_train_text.tolist())\n",
    "    X_val = get_cohere_embeddings(X_val_text.tolist())\n",
    "    X_test = get_cohere_embeddings(X_test_text.tolist())\n",
    "    \n",
    "    # Train the MLP classifier\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "    mlp.fit(X_train, Y_train)\n",
    "    \n",
    "    # Validate the model\n",
    "    Y_val_pred = mlp.predict(X_val)\n",
    "    val_accuracy = accuracy_score(Y_val, Y_val_pred)\n",
    "    print(f\"Validation Accuracy using Cohere embeddings: {val_accuracy:.4f}\")\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_eval_model_cohere((X_train_text, Y_train, X_val_text, Y_val, X_test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe vs Cohere\n",
    "Compare the performances of the two classifiers, and comment on your observations.\n",
    "The Validation Accuracy of the GloVe embeddings is 0.7263 while the validation accuracy of the cohere embeddings is 0.9263. This shows that cohere might be better at capturing the context of words in a sentence than GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
