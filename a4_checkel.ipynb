{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4: Large language models and in-context learning\n",
    "In this assignment, we are going to work with a large language model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generation and hyperparameters (10')\n",
    "1.1 Load a pre-trained language model, `meta-llama/Llama-3.2-1B`, using `AutoModelForCausalLM` class. This requires creating an account at huggingface and creating an access token. You'll also need to submit a request at [huggingface](https://huggingface.co/meta-llama/Llama-3.2-1B) to use it.\n",
    "An $X$ B model contains $X$ billion parameters. To run inference on this model, you'll need $4X$ GB memory on full precision. You can reduce it to $2X$ GB by specifying `torch_dtype=torch.float16` in the `from_pretrained()` step. If your computer does not have 2GB spare memory, you can use Google Colab or a smaller model like `gpt2-large`. To speed up the inference, you can load the model onto cuda (for the machines with NVidia GPU) or mps (for the machines with Apple Silicon).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\wheel\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wheel\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b28ee1b3c5f4bd0a3a5c906408a3a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae93d6e62f14171916b8fa4cb079f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5355a1bdab0a4b109dd793855152a5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "auth = \"\"\n",
    "\n",
    "os.environ[auth] = \"\"\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=os.environ[auth])\n",
    "\n",
    "modelname = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname, token=auth)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelname, \n",
    "    torch_dtype=torch.float16, \n",
    "    use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Use the built-in `generation` method. Starting from the first word of the sentence \"An apple a day keeps the doctors away\", generate up to 10 new tokens, using three methods: greedy, beam search, nucleus sampling. Following are the corresponding configurations:  \n",
    "- Greedy: `do_sample=False`    \n",
    "- Beam search: `num_beams=k`    \n",
    "- Nucleus sampling: `do_sample=True, top_p=0.8`  \n",
    "\n",
    "[The official doc](https://huggingface.co/docs/transformers/en/generation_strategies#customize-text-generation) briefly describes the generation modes, and [the GenerationConfig doc](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig) explains in details the generation parameters.  \n",
    "How long does it take to generate a sequence? What is the generation quality?  \n",
    "\n",
    "The documentation explains that modifying the decoding strategy can have an impact on the generation quality, possibly lowering repetition or forming more coherent sentences. The amount of time it takes to generate a sequence is based on the amount of tokens that need to be generated. For example in this exercise we are maxing the decoding at 10 tokens. There are also configuration options to end computation of a certain amount of time has passed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wheel\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\wheel\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>An 18-year-old woman presents to the emergency department\n",
      "Time taken: 17.331308841705322s\n"
     ]
    }
   ],
   "source": [
    "# Greedy search decoding\n",
    "import time\n",
    "\n",
    "input_text = \"An\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "start = time.time()\n",
    "outputs = model.generate(input_ids, max_new_tokens=10, do_sample=False)\n",
    "end = time.time()\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "print(f\"Time taken: {end - start}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sequence generation took around 17.33 seconds and has good sentence quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>An 8-year-old boy is brought to the physician\n",
      "Time taken: 93.51181674003601s\n"
     ]
    }
   ],
   "source": [
    "# TODO: Beam search decoding\n",
    "start = time.time()\n",
    "outputs = model.generate(input_ids, max_new_tokens=10, num_beams=5)\n",
    "end = time.time()\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "print(f\"Time taken: {end - start}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sequence generation took around 93.5 seconds and has good sentence quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Anatomy of a Killer: The Secret Science of At\n",
      "Time taken: 24.856630325317383s\n"
     ]
    }
   ],
   "source": [
    "# TODO: Nucleus sampling\n",
    "start = time.time()\n",
    "outputs = model.generate(input_ids, max_new_tokens=10, do_sample=True, top_p=0.8)\n",
    "end = time.time()\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "print(f\"Time taken: {end - start}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sentence generation took around 24.86 seconds and has good sentence quality however this time the intput text was continued to form the world Anatomy instead of keeping the first word as \"An\". This time it had also formed maybe a title of some type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Change the `temperature` generation parameter. Comment on the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>An Introduction to the Mathematics of Physics\n",
      "by Paul M\n",
      "Time taken: 20.635331392288208s\n"
     ]
    }
   ],
   "source": [
    "# TODO: Change the temperature parameter\n",
    "start = time.time()\n",
    "outputs = model.generate(input_ids, max_new_tokens=10, do_sample=True, top_p=0.8, temperature=0.7)\n",
    "end = time.time()\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "print(f\"Time taken: {end - start}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This senquence generation took around 20.64 seconds and has good sentence quality this time forming a title of some kind along with the author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Zero-shot classification using a language model (10')  \n",
    "Here we are going to turn the pre-trained language model into a classifier.  \n",
    "Give a classification question, an obvious solution is to prompt the model so that it only generates the answer. However, it's not always possible to let the LM generate the final answer. Following is a way: Frame the input text as \"[Problem] The answer is\". Query the probabilities of the next token being \"0\" vs \"1\" (or \"True\" vs \"False\", depending on the problem). Then, pass them through a softmax to get the probabilities. This is the probability of the model. Now you have an LM that can do the sentiment analysis task.  \n",
    "\n",
    "Pick N=10 questions in the validation set of SST in Assignment 1-2. Use this zero-shot classifier to solve them. Comment on the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "ds = load_dataset(\"glue\", \"sst2\")\n",
    "val_data = pd.DataFrame(ds[\"validation\"][:10])\n",
    "X_val_text = val_data[\"sentence\"]\n",
    "Y_val = val_data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "def zero_shot_learner(model, tokenizer, X_val_text, Y_val):\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(X_val_text)\n",
    "    \n",
    "    for text, true_label in zip(X_val_text, Y_val):\n",
    "        prompt_0 = f\"{text} The answer is 0.\"\n",
    "        prompt_1 = f\"{text} The answer is 1.\"\n",
    "        \n",
    "        inputs_0 = tokenizer(prompt_0, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs_1 = tokenizer(prompt_1, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs_0 = model(**inputs_0)\n",
    "            outputs_1 = model(**inputs_1)\n",
    "        \n",
    "        logits_0 = outputs_0.logits[0, -1, :]\n",
    "        logits_1 = outputs_1.logits[0, -1, :]\n",
    "        \n",
    "        prob_0 = F.softmax(logits_0, dim=-1)[tokenizer.convert_tokens_to_ids(\"0\")]\n",
    "        prob_1 = F.softmax(logits_1, dim=-1)[tokenizer.convert_tokens_to_ids(\"1\")]\n",
    "        \n",
    "        predicted_label = 0 if prob_0 > prob_1 else 1\n",
    "        \n",
    "        if predicted_label == true_label:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "accuracy = zero_shot_learner(model, tokenizer, X_val_text, Y_val)\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zero shot classifier had a validation accuracy of 0.7 meaning that it performs relatively well considering that it relies solely based on the pre-trained knowledge and has a generally good grasp of the semantics in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Few-shot in-context learning with language model (10')  \n",
    "How to avoid querying the probabilities? One popular method to improve the format-following behavior of the model using in-context learning (few-shot).  \n",
    "Few-shot learning places a few demonstration examples in the prompt. Each demonstration example follows the same format, for example: \"[Example Problem] The answer is False\\n\". Finally, the problem is concatenated into prompt: \"[Problem] The answer is\" \n",
    "Hopefully, through this few-shot demonstration, the model learns the format specification and can output the results directly. \n",
    "\n",
    "Randomlly pick $k=2$ examples in the SST's training set. Use this in-context learner to attempt the same set of N questions in the previous problem. Comment on the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.60\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def in_context_learner(model, tokenizer, X_val_text, Y_val):\n",
    "    \n",
    "    examples = random.sample(list(zip(X_val_text, Y_val)), 2)\n",
    "\n",
    "    demonstration_prompt = \"\"\n",
    "    for example_text, example_label in examples:\n",
    "        demonstration_prompt += f\"{example_text} The answer is {example_label}\\n\"\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(X_val_text)\n",
    "    \n",
    "    for text, true_label in zip(X_val_text, Y_val):\n",
    "        prompt = demonstration_prompt + f\"{text} The answer is\"\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=inputs.input_ids.shape[1] + 2,  # Allow space for the answer\n",
    "                pad_token_id=tokenizer.eos_token_id  # Handle padding for models like GPT-2\n",
    "            )\n",
    "        \n",
    "        # Decode the model's output\n",
    "        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract the predicted answer\n",
    "        predicted_label = output_text.split(\"The answer is\")[-1].strip().split()[0]\n",
    "        \n",
    "        # Compare with the true label\n",
    "        if predicted_label == str(true_label):\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "accuracy = in_context_learner(model, tokenizer, X_val_text, Y_val)\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The in context learner had a validation accuracy of 0.6 showing that it struggles a little more than the zero shot classifier. This could be due to the small sample size not being sufficiently diverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda)",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
