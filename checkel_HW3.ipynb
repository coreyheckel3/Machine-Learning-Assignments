{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYBvZS0vQ-gr"
   },
   "source": [
    "# CS 583-C-Homework 3\n",
    "## Neural networks\n",
    "\n",
    "---\n",
    "\n",
    "### ***Fill your details below***\n",
    "### Name: Corey Heckel\n",
    "### CWID: 10462028\n",
    "### Email ID: checkel\n",
    "### References: ***Cite your references here***\n",
    "\n",
    "\n",
    "---\n",
    "### Submission guidelines:\n",
    "\n",
    "#### 1. Submit this notebook along with its PDF version. You can do this by clicking File->Print->\"Save as PDF\"\n",
    "\n",
    "#### 2. Name the file as \"<mailID_HWnumber.extension>\".  \n",
    "\n",
    "For example, mailID is abcdefg @stevens.edu then name the files as abcdefg_HW1.ipynb and abcdefg_HW1.pdf.\n",
    "\n",
    "#### 3. Please do not Zip your files.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJQqrGTr4L4J"
   },
   "source": [
    "### Installing Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PtKvmZx-WmUu"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (87847310.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install torch\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Getk0l_zk3D"
   },
   "source": [
    "### Q1: (50 points)\n",
    "#### Design a vision transformer model and obtain an test accuracy above 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "H9j0igQj5FJx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "aNYrFg1g7VNO"
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size=28, patch_size=7, num_classes=10, embed_dim=256, num_heads=8, num_layers=6, mlp_dim=512):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.mlp_dim = mlp_dim\n",
    "        \n",
    "        self.patch_embed = nn.Conv2d(in_channels=1, out_channels=self.embed_dim, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        \n",
    "        self.encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim, nhead=self.num_heads, dim_feedforward=self.mlp_dim\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layers, num_layers=self.num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(self.embed_dim, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x) \n",
    "        x = x.flatten(2).transpose(1, 2) \n",
    "        \n",
    "        x = self.transformer_encoder(x) \n",
    "        \n",
    "        x = x.mean(dim=1)  \n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "eN-5U_BN5W0J"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_data = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_data = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=100, shuffle=False)\n",
    "\n",
    "model = VisionTransformer()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "mGfdZMvX5dIt"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return running_loss / len(train_loader), accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "T_k9rXQm5hvE"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.1057, Train Accuracy: 70.38%, Test Accuracy: 90.59%\n",
      "Epoch 2/10, Loss: 0.3578, Train Accuracy: 92.30%, Test Accuracy: 93.38%\n",
      "Epoch 3/10, Loss: 0.2252, Train Accuracy: 94.62%, Test Accuracy: 95.79%\n",
      "Epoch 4/10, Loss: 0.1706, Train Accuracy: 95.73%, Test Accuracy: 95.99%\n",
      "Epoch 5/10, Loss: 0.1396, Train Accuracy: 96.35%, Test Accuracy: 96.86%\n",
      "Epoch 6/10, Loss: 0.1169, Train Accuracy: 96.87%, Test Accuracy: 96.77%\n",
      "Epoch 7/10, Loss: 0.1036, Train Accuracy: 97.13%, Test Accuracy: 97.11%\n",
      "Epoch 8/10, Loss: 0.0895, Train Accuracy: 97.54%, Test Accuracy: 97.06%\n",
      "Epoch 9/10, Loss: 0.0804, Train Accuracy: 97.69%, Test Accuracy: 97.19%\n",
      "Epoch 10/10, Loss: 0.0718, Train Accuracy: 97.98%, Test Accuracy: 97.29%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, device)\n",
    "    test_accuracy = test(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP9Rr4Hl3ymp"
   },
   "source": [
    "### Q2: (50 points)\n",
    "#### Compared the accuracy with the accuracy of HW1.\n",
    "#### Discuss vision transformer's performance vs your network in HW1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy of the vision transformer is fairly similar to the network in HW1. In HW1 the accuracy was 97.76% and the vision transformer had an accuracy of 97.29%. It seems like with little change in test accuracy between the two models, it was observed that the vision transformer took significantly longer to run than the neural network due to its complex nature. That being said, it can be argued that a simpler model such as a neural network may be sufficient for some tasks such as optical character recognition while vision transformers may be better suited for much larger datasets where the advantages of the model could be observed better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Anaconda)",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
