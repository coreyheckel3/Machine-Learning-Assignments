{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Hands-on classifier model for NLP\n",
    "\n",
    "Welcome to the first assignment of CS 584! This assignment prepares you with some useful tools that are widely used in NLP. This assignment must be done individually.\n",
    "\n",
    "After this assignment, you should be able to:  \n",
    "1. Load a dataset from huggingface's dataset library, and do some exploratory analyses.  \n",
    "2. Use scikit-learn to build and train a feature-based model.  \n",
    "3. Use pytorch to build and train a feature-based model.  \n",
    "4. Use Optuna to automatically search for hyperparameters.  \n",
    "\n",
    "**Policy regarding Generative AI tools**: You may use Generative AI tools (GenAIs) -- including but not limited to GPT, Claude, Gemini, Cohere, etc., throughout the process of the assignment. Regardless of whether you use GenAI, you are responsible for the correctness of the contents you put into your assignment. If you use GenAI to polish the writing texts, no statement is necessary. If you use GenAI to generate the contents, when applicable, you should include the prompt template as an appendix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the dataset (5')\n",
    "First, we are going to load the datasets from huggingface's `datasets` library.\n",
    "Do some exploratory analysis on the dataset.  \n",
    "1.1 Print out one example in the dataset. Briefly comment on what it contains.  \n",
    "1.2 For each of the train, validation, and test set, compute the following statistics: \n",
    "- The number of data samples with each class label.  \n",
    "- The mean and std of the sentence lengths (in words) of each `question`.  \n",
    "\n",
    "1.3 Vectorize the validation set of the dataset, following the approaches specified in the train set example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\wheel\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from datasets) (2024.6.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from datasets) (0.24.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\wheel\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\wheel\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (1.21.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->datasets) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from datasets import load_dataset  # huggingface datasets\n",
    "\n",
    "ds = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from the training set: {'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0}\n"
     ]
    }
   ],
   "source": [
    "# TODO -- Print out one example in the dataset. Briefly comment on what it contains.\n",
    "example = ds['train'][0]\n",
    "print(f\"Example from the training set: {example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example of the dataset includes a sentence (in this case it is \"hide new secretions from the parental units\"), a label (in this case, 0), and an index (in this case 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(ds[\"train\"])\n",
    "X_train_text = train_data[\"sentence\"]\n",
    "Y_train_sk = train_data[\"label\"]\n",
    "\n",
    "val_data = pd.DataFrame(ds[\"validation\"])\n",
    "X_val_text = val_data[\"sentence\"]\n",
    "Y_val_sk = val_data[\"label\"]\n",
    "\n",
    "test_data = pd.DataFrame(ds[\"test\"])\n",
    "X_test_text = test_data[\"sentence\"]\n",
    "Y_test = test_data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Stats: {'distribution': label\n",
      "1    37569\n",
      "0    29780\n",
      "Name: count, dtype: int64, 'sentence_length_avg': 9.409553222765, 'sentence_length_std': 8.073806407501392}\n",
      "Validation Stats: {'distribution': label\n",
      "1    444\n",
      "0    428\n",
      "Name: count, dtype: int64, 'sentence_length_avg': 19.548165137614678, 'sentence_length_std': 8.76390003460537}\n",
      "Test Stats: {'distribution': label\n",
      "-1    1821\n",
      "Name: count, dtype: int64, 'sentence_length_avg': 19.233937397034598, 'sentence_length_std': 8.922386423395173}\n"
     ]
    }
   ],
   "source": [
    "# TODO -- compute the exploratory statistics\n",
    "train_statistics = {\n",
    "    \"distribution\": Y_train_sk.value_counts(),\n",
    "    \"sentence_length_avg\": X_train_text.apply(lambda x: len(x.split())).mean(),\n",
    "    \"sentence_length_std\": X_train_text.apply(lambda x: len(x.split())).std()\n",
    "}\n",
    "\n",
    "validation_statistics = {\n",
    "    \"distribution\": Y_val_sk.value_counts(),\n",
    "    \"sentence_length_avg\": X_val_text.apply(lambda x: len(x.split())).mean(),\n",
    "    \"sentence_length_std\": X_val_text.apply(lambda x: len(x.split())).std()\n",
    "}\n",
    "\n",
    "test_statistics = {\n",
    "    \"distribution\": Y_test.value_counts(),\n",
    "    \"sentence_length_avg\": X_test_text.apply(lambda x: len(x.split())).mean(),\n",
    "    \"sentence_length_std\": X_test_text.apply(lambda x: len(x.split())).std()\n",
    "}\n",
    "\n",
    "print(\"Training Stats:\", train_statistics)\n",
    "print(\"Validation Stats:\", validation_statistics)\n",
    "print(\"Test Stats:\", test_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to vectorize the texts using TfidfVectorizer, then compute the Tf-idf features.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3120\n",
      "(67349, 3120)\n",
      "(67349, 3120)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer \n",
    "from sklearn.neural_network import MLPClassifier \n",
    "\n",
    "counter = CountVectorizer(min_df=10, max_df=20) \n",
    "counter.fit(X_train_text)\n",
    "print(\"Vocabulary size:\", len(counter.vocabulary_))\n",
    "X_train_counts = counter.transform(X_train_text)\n",
    "print(X_train_counts.shape) \n",
    "count2tfidf = TfidfTransformer(use_idf=True).fit(X_train_counts)\n",
    "X_train_sk = count2tfidf.transform(X_train_counts).toarray()\n",
    "print(X_train_sk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Use the counter to convert X_val_text to occurrence vectors\n",
    "# Note: don't create a new CountVectorizer, as we want to compute the vocabulary only on the train set\n",
    "X_val_counts = counter.transform(X_val_text) \n",
    "\n",
    "# TODO - use count2tfidf to transform the counts into Tfidf features\n",
    "# Note: don't create a new TfidfTransformer\n",
    "X_val_sk = count2tfidf.transform(X_val_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train scikit-learn models (10')\n",
    "Train a two-layer MLPClassifier using `random_state=0`. Manually tune the hyperparameters on the validation set. Report the procedure of hyperparameter tuning. Specifically: report the hyperparameters you have tried, and their results.  \n",
    "\n",
    "After you are satisfied with the validation set performances, report the validation set performance. Use this set of hyperparameters and repeat the model training procedure for five times using `random_state` as 1, 2, 3, 31, 42 respectively. Record the five accuracy numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.5826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5825688073394495"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starter\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_sklearn_model(X_train, Y_train, X_val, Y_val, random_state):\n",
    "    mlp_model = MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        max_iter=200,\n",
    "        learning_rate_init=0.001,\n",
    "        alpha=0.000001,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    mlp_model.fit(X_train, Y_train)\n",
    "\n",
    "    Y_val_pred = mlp_model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(Y_val, Y_val_pred)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    return val_accuracy\n",
    "\n",
    "train_sklearn_model(X_train_sk, Y_train_sk, X_val_sk, Y_val_sk, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters: hl:(100,50) mi:300 lr:0.01 alpha:0.0001 Accuracy: 0.5677\n",
    "\n",
    "Parameters: hl:(200,100) mi:300 lr:0.01 alpha:0.0001 Accuracy: 0.5791\n",
    "\n",
    "Parameters: hl:(200,100) mi:500 lr:0.01 alpha:0.0001 Accuracy: 0.5791\n",
    "\n",
    "Parameters: hl:(200,100) mi:500 lr:0.001 alpha:0.0001 Accuracy:0.5814\n",
    "\n",
    "Parameters: hl:(150,75) mi:500 lr:0.001 alpha:0.0001 Accuracy:0.5837\n",
    "\n",
    "Parameters: hl:(150,75) mi:500 lr:0.01 alpha:0.0001 Accuracy:0.5700\n",
    "\n",
    "Parameters: hl:(150,75) mi:500 lr:0.001 alpha:0.0001 Accuracy:0.5826\n",
    "\n",
    "Parameters: hl:(150,75) mi:500 lr:0.001 alpha:0.00001 Accuracy:0.5849\n",
    "\n",
    "Parameters: hl:(150,75) mi:500 lr:0.001 alpha:0.00001 Accuracy:0.5883\n",
    "\n",
    "Parameters: hl:(150,75) mi:500 lr:0.001 alpha:0.000001 Accuracy:0.5849\n",
    "\n",
    "Parameters: hl:(100,50) mi:200 lr:0.001 alpha:0.000001 Accuracy: 0.5894\n",
    "\n",
    "Best: Parameters: hl:(100,50) mi:200 lr:0.001 alpha:0.000001 Accuracy:0.5894\n",
    "\n",
    "Random State 1: Accuracy: 0.5791\n",
    "\n",
    "Random State 2: Accuracy: 0.5780\n",
    "\n",
    "Random State 3: Accuracy: 0.5757\n",
    "\n",
    "Random State 31: Accuracy: 0.5803\n",
    "\n",
    "Random State 42: Accuracy: 0.5826"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train a pytorch model (10')\n",
    "Here you will repeat the training of a two-layer fully-connected neural network using pytorch. Following are some specifications that may be helpful:  \n",
    "- For each of the train and validation set, specify a dataloader, preferrably using `torch.utils.data.DataLoader`.  \n",
    "- Use an optimizer of your choice. Adam, AdamW and SGD are popular choices.  \n",
    "- Designate a number, `train_epochs`, as the number of passes through the dataset during training. Each pass through the training dataset is called an epoch.  \n",
    "  - During the epoch, there may be many steps. In each step, load a batch of data from the dataloader. Compute the loss. Do a `backward()` pass to compute the gradients. Call a `step()` from the optimizer to update the model's parameters. Then zero out the gradients.\n",
    "- At the end of each epoch, go through a validation run. Do *not* optimize the model during the validation run. Compute the accuracy of the model on this validation run, and print it out.\n",
    "\n",
    "Tune the hyperparameters on the validation set. Report the hyperparameters you have tried, and their results. \n",
    "\n",
    "After you are satisfied with the validation set performances, record the set of hyperparameters. Use this set of hyperparameters, and repeat the model training procedure for five times using 1, 2, 3, 31, 42 as random seeds respectively. You can use `torch.manual_seed()` to set the random seeds. Record the five accuracy numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n",
      "Epoch 1, val accuracy 0.5803\n",
      "Epoch 2, val accuracy 0.5872\n",
      "Epoch 3, val accuracy 0.5849\n",
      "Epoch 4, val accuracy 0.5814\n",
      "Epoch 5, val accuracy 0.5872\n",
      "Epoch 6, val accuracy 0.5837\n",
      "Epoch 7, val accuracy 0.5849\n",
      "Epoch 8, val accuracy 0.5872\n",
      "Epoch 9, val accuracy 0.5872\n",
      "Epoch 10, val accuracy 0.5894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5894495412844036"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from datasets import load_dataset\n",
    "from collections import OrderedDict\n",
    "\n",
    "X_train_pt = torch.tensor(X_train_sk).float()\n",
    "Y_train_pt = torch.tensor(Y_train_sk.values).long()\n",
    "X_val_pt = torch.tensor(X_val_sk.toarray()).float()\n",
    "Y_val_pt = torch.tensor(Y_val_sk.values).long()\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, all_layer_sizes):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(all_layer_sizes) - 1):\n",
    "            layers.append(('linear' + str(i), nn.Linear(all_layer_sizes[i], all_layer_sizes[i + 1])))\n",
    "            if i < len(all_layer_sizes) - 2:\n",
    "                layers.append(('relu' + str(i), nn.ReLU()))\n",
    "        self.net = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "def my_collate_function(batch):\n",
    "    batch_X, batch_Y = [], []\n",
    "    for item in batch:\n",
    "        input_ids, label = item\n",
    "        batch_X.append(input_ids)\n",
    "        batch_Y.append(label)\n",
    "    return torch.stack(batch_X).float(), torch.tensor(batch_Y).long()\n",
    "\n",
    "def prepare_zipped_XY(X,Y):\n",
    "    zipped = []\n",
    "    for i in range(len(X)):\n",
    "         zipped.append((X[i], Y[i]))\n",
    "    return zipped\n",
    "\n",
    "def train_pytorch_model(X_train, Y_train, X_val, Y_val, seed):\n",
    "    # Define the manual seed\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Hyperparameters\n",
    "    train_epochs = 10\n",
    "    batch_size = 50\n",
    "    learning_rate = 0.00077724906859174\n",
    "    input_size = 3120 \n",
    "    hidden_sizes = [input_size, 216, 191, 2] \n",
    "            \n",
    "    # Set up the model, optimizer, and dataloaders\n",
    "    model = MLP(hidden_sizes)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_dataloader = DataLoader(prepare_zipped_XY(X_train, Y_train), batch_size=batch_size, collate_fn=my_collate_function, shuffle=True)\n",
    "    val_dataloader = DataLoader(prepare_zipped_XY(X_val, Y_val), batch_size=batch_size, collate_fn=my_collate_function, shuffle=False)\n",
    "    \n",
    "    print(\"Start training!\")\n",
    "    last_epoch_dev_acc = 0\n",
    "    for epoch in range(train_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_Y in train_dataloader:\n",
    "            optim.zero_grad()\n",
    "            logits = model(batch_X)\n",
    "            loss = loss_fn(logits, batch_Y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        model.eval()\n",
    "        n_correct, n_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_Y in val_dataloader:\n",
    "                logits = model(batch_X)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                n_correct += (predictions == batch_Y).sum().item()\n",
    "                n_total += batch_Y.size(0)\n",
    "        \n",
    "        last_epoch_dev_acc = n_correct / n_total\n",
    "        print(f\"Epoch {epoch+1}, val accuracy {last_epoch_dev_acc:.4f}\")\n",
    "    \n",
    "    return last_epoch_dev_acc\n",
    "\n",
    "train_pytorch_model(X_train_pt, Y_train_pt, X_val_pt, Y_val_pt, 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epochs: 10 Batch size: 32 lr: 0.001 hidden sizes: [input_size, 64, 32, 2] Accuracy: 0.5849\n",
    "\n",
    "Epochs: 10 Batch size: 16 lr: 0.001 hidden sizes: [input_size, 64, 32, 2] Accuracy: 0.5791\n",
    "\n",
    "Epochs: 10 Batch size: 64 lr: 0.001 hidden sizes: [input_size, 64, 32, 2] Accuracy: 0.5791\n",
    "\n",
    "Epochs: 10 Batch size: 64 lr: 0.001 hidden sizes: [input_size, 64, 32, 2] Accuracy: 0.5757\n",
    "\n",
    "Epochs: 10 Batch size: 64 lr: 0.01 hidden sizes: [input_size, 64, 32, 2] Accuracy: 0.5872\n",
    "\n",
    "Epochs: 10 Batch size: 64 lr: 0.01 hidden sizes: [input_size, 64, 32, 2] Accuracy: 0.5860\n",
    "\n",
    "Epochs: 10 Batch size: 64 lr: 0.01 hidden sizes: [input_size, 50, 25, 2] Accuracy: 0.5872\n",
    "\n",
    "Epochs: 5 Batch size: 64 lr: 0.01 hidden sizes: [input_size, 50, 25, 2] Accuracy: 0.5894\n",
    "\n",
    "Epochs: 7 Batch size: 64 lr: 0.01 hidden sizes: [input_size, 50, 25, 2] Accuracy: 0.5929\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Best:  Epochs: 7 Batch size: 64 lr: 0.01 hidden sizes: [input_size, 50, 25, 2] Accuracy: 0.5929\n",
    "    \n",
    "\n",
    "Random Seed 1: Accuracy: 0.5929\n",
    "\n",
    "Random Seed 2: Accuracy: 0.5711\n",
    "\n",
    "Random Seed 3: Accuracy: 0.5768\n",
    "\n",
    "Random Seed 31: Accuracy:0.5860\n",
    "\n",
    "Random Seed 42: Accuracy:0.5768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hyperparameter tuning (10')\n",
    "This question requires modifying your previous pytorch training scripts. Use Optuna to find the hyperparameters that can maximize the accuracy on the validation set.  \n",
    "\n",
    "The range of hyperparameters don't need to be too large (i.e., the total program should still be runnable within a reasonable time). The most important hyperparameter is the learning rate. Other hyperparameters that you can tune include the train epochs, batch size, hidden sizes, etc.  \n",
    "\n",
    "When you are satisfied with the hyperparameters, report the hyperparameter and the resulting validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 23:43:25,703] A new study created in memory with name: no-name-69abf319-7094-4c07-8c0e-e94ec81ba62d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\wheel\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\wheel\\appdata\\roaming\\python\\python39\\site-packages (from optuna) (1.21.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from optuna) (1.13.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: colorlog in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from optuna) (1.4.32)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from packaging>=20.0->optuna) (3.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\wheel\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 23:45:55,214] Trial 0 finished with value: 0.5825688073394495 and parameters: {'train_epochs': 18, 'batch_size': 30, 'learning_rate': 0.000289562399213477, 'hidden_layer_1': 54, 'hidden_layer_2': 52}. Best is trial 0 with value: 0.5825688073394495.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 23:47:42,299] Trial 1 finished with value: 0.591743119266055 and parameters: {'train_epochs': 14, 'batch_size': 40, 'learning_rate': 1.0311981957320559e-05, 'hidden_layer_1': 108, 'hidden_layer_2': 22}. Best is trial 1 with value: 0.591743119266055.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 23:49:40,923] Trial 2 finished with value: 0.588302752293578 and parameters: {'train_epochs': 14, 'batch_size': 21, 'learning_rate': 1.5880526469912245e-05, 'hidden_layer_1': 16, 'hidden_layer_2': 182}. Best is trial 1 with value: 0.591743119266055.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 23:51:35,377] Trial 3 finished with value: 0.5940366972477065 and parameters: {'train_epochs': 10, 'batch_size': 50, 'learning_rate': 0.00077724906859174, 'hidden_layer_1': 216, 'hidden_layer_2': 191}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 23:53:36,341] Trial 4 finished with value: 0.5768348623853211 and parameters: {'train_epochs': 13, 'batch_size': 24, 'learning_rate': 2.4983518713502087e-05, 'hidden_layer_1': 23, 'hidden_layer_2': 98}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 23:56:08,391] Trial 5 finished with value: 0.5860091743119266 and parameters: {'train_epochs': 6, 'batch_size': 21, 'learning_rate': 0.000687220772244092, 'hidden_layer_1': 165, 'hidden_layer_2': 157}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 23:59:09,757] Trial 6 finished with value: 0.5791284403669725 and parameters: {'train_epochs': 12, 'batch_size': 24, 'learning_rate': 0.00023596851785998528, 'hidden_layer_1': 119, 'hidden_layer_2': 27}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-24 00:04:50,452] Trial 7 finished with value: 0.5825688073394495 and parameters: {'train_epochs': 19, 'batch_size': 35, 'learning_rate': 0.0003272733559464876, 'hidden_layer_1': 255, 'hidden_layer_2': 84}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-24 00:10:40,409] Trial 8 finished with value: 0.5905963302752294 and parameters: {'train_epochs': 17, 'batch_size': 20, 'learning_rate': 0.000612330757673899, 'hidden_layer_1': 146, 'hidden_layer_2': 191}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-24 00:12:36,391] Trial 9 finished with value: 0.5779816513761468 and parameters: {'train_epochs': 13, 'batch_size': 59, 'learning_rate': 0.0005684923019968243, 'hidden_layer_1': 199, 'hidden_layer_2': 77}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-24 00:13:48,712] Trial 10 finished with value: 0.5837155963302753 and parameters: {'train_epochs': 7, 'batch_size': 53, 'learning_rate': 7.422913457332706e-05, 'hidden_layer_1': 237, 'hidden_layer_2': 247}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-24 00:14:52,638] Trial 11 finished with value: 0.5768348623853211 and parameters: {'train_epochs': 10, 'batch_size': 45, 'learning_rate': 5.9863428312439985e-05, 'hidden_layer_1': 96, 'hidden_layer_2': 240}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-24 00:16:18,152] Trial 12 finished with value: 0.5940366972477065 and parameters: {'train_epochs': 9, 'batch_size': 45, 'learning_rate': 1.0483681125556494e-05, 'hidden_layer_1': 198, 'hidden_layer_2': 124}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-24 00:17:40,886] Trial 13 finished with value: 0.573394495412844 and parameters: {'train_epochs': 9, 'batch_size': 49, 'learning_rate': 3.3280592984599066e-05, 'hidden_layer_1': 196, 'hidden_layer_2': 123}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-24 00:18:55,546] Trial 14 finished with value: 0.5825688073394495 and parameters: {'train_epochs': 9, 'batch_size': 64, 'learning_rate': 0.0001587326247232422, 'hidden_layer_1': 213, 'hidden_layer_2': 207}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-24 00:20:25,921] Trial 15 finished with value: 0.5814220183486238 and parameters: {'train_epochs': 11, 'batch_size': 54, 'learning_rate': 0.00012537037131652142, 'hidden_layer_1': 172, 'hidden_layer_2': 139}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-24 00:21:23,536] Trial 16 finished with value: 0.5802752293577982 and parameters: {'train_epochs': 5, 'batch_size': 42, 'learning_rate': 3.9909281242346943e-05, 'hidden_layer_1': 223, 'hidden_layer_2': 158}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-24 00:23:28,129] Trial 17 finished with value: 0.5871559633027523 and parameters: {'train_epochs': 8, 'batch_size': 34, 'learning_rate': 0.000995343660874508, 'hidden_layer_1': 177, 'hidden_layer_2': 216}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-24 00:26:33,198] Trial 18 finished with value: 0.5745412844036697 and parameters: {'train_epochs': 16, 'batch_size': 46, 'learning_rate': 1.5229222632851068e-05, 'hidden_layer_1': 255, 'hidden_layer_2': 115}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-24 00:27:50,440] Trial 19 finished with value: 0.5802752293577982 and parameters: {'train_epochs': 11, 'batch_size': 52, 'learning_rate': 7.583888927018843e-05, 'hidden_layer_1': 144, 'hidden_layer_2': 157}. Best is trial 3 with value: 0.5940366972477065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: FrozenTrial(number=3, state=TrialState.COMPLETE, values=[0.5940366972477065], datetime_start=datetime.datetime(2024, 9, 23, 23, 49, 40, 924017), datetime_complete=datetime.datetime(2024, 9, 23, 23, 51, 35, 376886), params={'train_epochs': 10, 'batch_size': 50, 'learning_rate': 0.00077724906859174, 'hidden_layer_1': 216, 'hidden_layer_2': 191}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'train_epochs': IntDistribution(high=20, log=False, low=5, step=1), 'batch_size': IntDistribution(high=64, log=False, low=16, step=1), 'learning_rate': FloatDistribution(high=0.001, log=True, low=1e-05, step=None), 'hidden_layer_1': IntDistribution(high=255, log=False, low=16, step=1), 'hidden_layer_2': IntDistribution(high=255, log=False, low=16, step=1)}, trial_id=3, value=None)\n",
      "Best hyperparameters: {'train_epochs': 10, 'batch_size': 50, 'learning_rate': 0.00077724906859174, 'hidden_layer_1': 216, 'hidden_layer_2': 191}\n"
     ]
    }
   ],
   "source": [
    "# Starter\n",
    "!pip install optuna\n",
    "import optuna \n",
    "\n",
    "def train_pytorch_model_with_optuna(trial, X_train, Y_train, X_val, Y_val):\n",
    "    train_epochs = trial.suggest_int('train_epochs', 5, 20)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 64)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    hidden_layer_1 = trial.suggest_int('hidden_layer_1', 16, 255)\n",
    "    hidden_layer_2 = trial.suggest_int('hidden_layer_2', 16, 255)\n",
    "\n",
    "    input_size = 3120 \n",
    "    hidden_sizes = [input_size, hidden_layer_1, hidden_layer_2, 2]\n",
    "\n",
    "    model = MLP(hidden_sizes)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_dataloader = DataLoader(prepare_zipped_XY(X_train, Y_train), batch_size=batch_size, collate_fn=my_collate_function, shuffle=True)\n",
    "    val_dataloader = DataLoader(prepare_zipped_XY(X_val, Y_val), batch_size=batch_size, collate_fn=my_collate_function, shuffle=False)\n",
    "    \n",
    "    print(\"Start training!\")\n",
    "    last_epoch_dev_acc = 0\n",
    "    for epoch in range(train_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_Y in train_dataloader:\n",
    "            optim.zero_grad()\n",
    "            logits = model(batch_X)\n",
    "            loss = loss_fn(logits, batch_Y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        model.eval()\n",
    "        n_correct, n_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_Y in val_dataloader:\n",
    "                logits = model(batch_X)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                n_correct += (predictions == batch_Y).sum().item()\n",
    "                n_total += batch_Y.size(0)\n",
    "        \n",
    "        last_epoch_dev_acc = n_correct / n_total\n",
    "    \n",
    "    return last_epoch_dev_acc\n",
    "\n",
    "\n",
    "def find_optimal_hyper_params(X_train, Y_train, X_val, Y_val):\n",
    "    def objective(trial):\n",
    "        return train_pytorch_model_with_optuna(trial, X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "    # Start an Optuna study\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    print(f\"Best trial: {study.best_trial}\")\n",
    "    print(f\"Best hyperparameters: {study.best_params}\")\n",
    "\n",
    "find_optimal_hyper_params(X_train_pt, Y_train_pt, X_val_pt, Y_val_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best hyperparameters: {'train_epochs': 10, 'batch_size': 50, 'learning_rate': 0.00077724906859174, 'hidden_layer_1': 216, 'hidden_layer_2': 191}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Bonus: Compare the performances of the two methods (2')\n",
    "Use an appropriate $t$ test, compare the five performance numbers of the sklearn model and the pytorch model *under the same set of hyperparameters*. Do their results differ?\n",
    "\n",
    "Note: The scores for bonus will be added to the A1 total score, but the total score will be capped to 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\wheel\\anaconda3\\lib\\site-packages (1.7.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\users\\wheel\\appdata\\roaming\\python\\python39\\site-packages (from scipy) (1.21.0)\n",
      "Validation Accuracy: 0.5872\n",
      "Start training!\n",
      "Epoch 1, val accuracy 0.5814\n",
      "Epoch 2, val accuracy 0.5745\n",
      "Epoch 3, val accuracy 0.5780\n",
      "Epoch 4, val accuracy 0.5791\n",
      "Epoch 5, val accuracy 0.5883\n",
      "Epoch 6, val accuracy 0.5837\n",
      "Epoch 7, val accuracy 0.5780\n",
      "Epoch 8, val accuracy 0.5826\n",
      "Epoch 9, val accuracy 0.5814\n",
      "Epoch 10, val accuracy 0.5894\n",
      "Validation Accuracy: 0.5826\n",
      "Start training!\n",
      "Epoch 1, val accuracy 0.5791\n",
      "Epoch 2, val accuracy 0.5791\n",
      "Epoch 3, val accuracy 0.5837\n",
      "Epoch 4, val accuracy 0.5883\n",
      "Epoch 5, val accuracy 0.5849\n",
      "Epoch 6, val accuracy 0.5883\n",
      "Epoch 7, val accuracy 0.5894\n",
      "Epoch 8, val accuracy 0.5849\n",
      "Epoch 9, val accuracy 0.5917\n",
      "Epoch 10, val accuracy 0.5883\n",
      "Validation Accuracy: 0.5826\n",
      "Start training!\n",
      "Epoch 1, val accuracy 0.5894\n",
      "Epoch 2, val accuracy 0.5849\n",
      "Epoch 3, val accuracy 0.5872\n",
      "Epoch 4, val accuracy 0.5768\n",
      "Epoch 5, val accuracy 0.5826\n",
      "Epoch 6, val accuracy 0.5837\n",
      "Epoch 7, val accuracy 0.5814\n",
      "Epoch 8, val accuracy 0.5906\n",
      "Epoch 9, val accuracy 0.5872\n",
      "Epoch 10, val accuracy 0.5803\n",
      "Validation Accuracy: 0.5791\n",
      "Start training!\n",
      "Epoch 1, val accuracy 0.5803\n",
      "Epoch 2, val accuracy 0.5814\n",
      "Epoch 3, val accuracy 0.5872\n",
      "Epoch 4, val accuracy 0.5883\n",
      "Epoch 5, val accuracy 0.5803\n",
      "Epoch 6, val accuracy 0.5894\n",
      "Epoch 7, val accuracy 0.5917\n",
      "Epoch 8, val accuracy 0.5906\n",
      "Epoch 9, val accuracy 0.5883\n",
      "Epoch 10, val accuracy 0.5883\n",
      "Validation Accuracy: 0.5849\n",
      "Start training!\n",
      "Epoch 1, val accuracy 0.5757\n",
      "Epoch 2, val accuracy 0.5837\n",
      "Epoch 3, val accuracy 0.5872\n",
      "Epoch 4, val accuracy 0.5780\n",
      "Epoch 5, val accuracy 0.5860\n",
      "Epoch 6, val accuracy 0.5894\n",
      "Epoch 7, val accuracy 0.5860\n",
      "Epoch 8, val accuracy 0.5860\n",
      "Epoch 9, val accuracy 0.5906\n",
      "Epoch 10, val accuracy 0.5883\n",
      "Sklearn Model Scores: [0.5871559633027523, 0.5825688073394495, 0.5825688073394495, 0.5791284403669725, 0.5848623853211009]\n",
      "PyTorch Model Scores: [0.5894495412844036, 0.588302752293578, 0.5802752293577982, 0.588302752293578, 0.588302752293578]\n",
      "t-statistic: -1.9331906986565806, p-value: 0.12535460853443622\n",
      "The results are not significantly different.\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "\n",
    "def compare_performances(X_train_sk, Y_train_sk, X_val_sk, Y_val_sk, X_train_pt, Y_train_pt, X_val_pt, Y_val_pt, n_runs=5):\n",
    "    sklearn_scores = []\n",
    "    pytorch_scores = []\n",
    "    \n",
    "    for _ in range(n_runs):\n",
    "        seed = random.randint(0, 10000)\n",
    "\n",
    "        sklearn_acc = train_sklearn_model(X_train_sk, Y_train_sk, X_val_sk, Y_val_sk, seed)\n",
    "        sklearn_scores.append(sklearn_acc)\n",
    "\n",
    "        pytorch_acc = train_pytorch_model(X_train_pt, Y_train_pt, X_val_pt, Y_val_pt, seed)\n",
    "        pytorch_scores.append(pytorch_acc)\n",
    "\n",
    "    t_stat, p_value = ttest_rel(sklearn_scores, pytorch_scores)\n",
    "    print(f\"Sklearn Model Scores: {sklearn_scores}\")\n",
    "    print(f\"PyTorch Model Scores: {pytorch_scores}\")\n",
    "    print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(\"The results are significantly different.\")\n",
    "    else:\n",
    "        print(\"The results are not significantly different.\")\n",
    "\n",
    "\n",
    "\n",
    "compare_performances(X_train_sk, Y_train_sk, X_val_sk, Y_val_sk, X_train_pt, Y_train_pt, X_val_pt, Y_val_pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
